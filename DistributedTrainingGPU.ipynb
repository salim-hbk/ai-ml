{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DistributedTrainingGPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM46tQ0Xlasy1BLuuM4nmMY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salim-hbk/ai-ml/blob/main/DistributedTrainingGPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4yMphPmAtr5"
      },
      "source": [
        "**Parallel distributed training**\r\n",
        "\r\n",
        "* In this architecture, we create multiple instances of same model, place it in different devices, here device means CPU or accelerators like GPU, TPU, these device can dwell in same computer or distributed across network\r\n",
        "* Then we create different slices of datasets to feed into different devices, note that different models in different devices might be initialized with different weights so we need to have aggregate these weights during training\r\n",
        "* Mirrored variables are those variables that have common stored values that it contains or updated by the 2 or more models.\r\n",
        "* workers is a software where training takes place\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DNLZFNvB_lm"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIGcsATe2xVw",
        "outputId": "3b1f7f7e-5fea-4ee2-cf47-a230c0140545"
      },
      "source": [
        "import os\r\n",
        "# Note that it generally has a minimum of 8 cores, but if your GPU has\r\n",
        "# less, you need to set this. In this case one of my GPUs has 4 cores\r\n",
        "os.environ[\"TF_MIN_GPU_MULTIPROCESSOR_COUNT\"] = \"4\"\r\n",
        "\r\n",
        "# If the list of devices is not specified in the\r\n",
        "# `tf.distribute.MirroredStrategy` constructor, it will be auto-detected.\r\n",
        "# If you have *different* GPUs in your system, you probably have to set up cross_device_ops like this\r\n",
        "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\r\n",
        "print(f'Number of devices {strategy.num_replicas_in_sync}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Number of devices 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiZXr-KnyyQN"
      },
      "source": [
        "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.fashion_mnist.load_data()\r\n",
        "\r\n",
        "train_x = np.reshape(train_x, [-1, 28,28,1])\r\n",
        "test_x =  np.reshape(test_x, [-1, 28,28,1])\r\n",
        "train_x = train_x/255.\r\n",
        "test_x = test_x/255."
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfG2GBFH03eU"
      },
      "source": [
        "BUFFER_SIZE = len(train_x)\r\n",
        "BATCH_SIZE_PER_REPLICA = 64\r\n",
        "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRBt0okv4ivI"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\r\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(GLOBAL_BATCH_SIZE)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROESJv8k5OCH"
      },
      "source": [
        "Next we need to create distributed dataset to run on parallel devices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO9DiYsF5GC4"
      },
      "source": [
        "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\r\n",
        "test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nj6j5TE5o-r"
      },
      "source": [
        "def base_model():\r\n",
        "  input = tf.keras.layers.Input(shape=[28,28,1,])\r\n",
        "  x = tf.keras.layers.Conv2D(32, 3, activation='relu')(input)\r\n",
        "  x = tf.keras.layers.MaxPooling2D()(x)\r\n",
        "  x = tf.keras.layers.Conv2D(64, 3, activation='relu')(x)\r\n",
        "  x = tf.keras.layers.MaxPooling2D()(x)\r\n",
        "  x = tf.keras.layers.Flatten()(x)\r\n",
        "  x = tf.keras.layers.Dense(64, activation='relu')(x)\r\n",
        "  output = tf.keras.layers.Dense(10, activation='softmax')(x)\r\n",
        "  model = tf.keras.models.Model(inputs=input, outputs=output)\r\n",
        "  return model  "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu_xvsaQ66sQ"
      },
      "source": [
        "* Instead of model.compile, we're going to do custom training, so let's do that\r\n",
        "* within a strategy scope\r\n",
        "* We will use sparse categorical crossentropy as always. But, instead of having the loss function\r\n",
        "* manage the map reduce across GPUs for us, we'll do it ourselves with a simple algorithm.\r\n",
        "* the map reduce is how the losses get aggregated\r\n",
        "* Set reduction to `none` so we can do the reduction afterwards and divide byglobal batch size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ev-lyEx61S9"
      },
      "source": [
        "with strategy.scope():\r\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\r\n",
        "\r\n",
        "  def compute_loss(labels, predictions):\r\n",
        "    per_example_loss = loss_object(labels, predictions)\r\n",
        "    print(per_example_loss)\r\n",
        "    return tf.nn.compute_average_loss(per_example_loss=per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\r\n",
        "\r\n",
        "  test_loss = tf.keras.metrics.Mean(name='test_loss')\r\n",
        "\r\n",
        "  # Accuracy on train and test will be SparseCategoricalAccuracy\r\n",
        "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\r\n",
        "  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\r\n",
        "\r\n",
        "  optimizer = tf.keras.optimizers.Adam()\r\n",
        "  model = base_model()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0hVgCur9vg_"
      },
      "source": [
        "###########################\r\n",
        "# Training Steps Functions\r\n",
        "###########################\r\n",
        "* `run` replicates the provided computation and runs it\r\n",
        "* with the distributed input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf1uW2eN9ckX"
      },
      "source": [
        "@tf.function\r\n",
        "def distributed_train_step(dataset):\r\n",
        "  per_replica_losses = strategy.run(train_step, args=(dataset,) )\r\n",
        "  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\r\n",
        "\r\n",
        "\r\n",
        "def train_step(dataset):\r\n",
        "  inputs, labels = dataset\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    prediction = model(inputs, training=True)\r\n",
        "    loss = compute_loss(labels, prediction)\r\n",
        "\r\n",
        "  gradient =  tape.gradient(loss, model.trainable_variables)\r\n",
        "  optimizer.apply_gradients(zip(gradient, model.trainable_variables))\r\n",
        "\r\n",
        "  train_accuracy.update_state(labels, prediction)\r\n",
        "  return loss\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def distributed_test_step(test_inputs):\r\n",
        "  return strategy.run(test_step, args=(test_inputs,))\r\n",
        "\r\n",
        "\r\n",
        "def test_step(data_input):\r\n",
        "  data, label = data_input\r\n",
        "  logits =  model(data)\r\n",
        "  loss = loss_object(label, logits)\r\n",
        "\r\n",
        "  test_loss.update_state(loss)\r\n",
        "  test_accuracy.update_state(label, logits)\r\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoYRhynzBn-z",
        "outputId": "1c56f25f-b811-4e1d-f1bb-4822c673cb49"
      },
      "source": [
        "epochs = 10\r\n",
        "\r\n",
        "for epoch in range(epochs):\r\n",
        "  total_loss =0.\r\n",
        "  num_batches = 0\r\n",
        "\r\n",
        "  for batch in train_dist_dataset:\r\n",
        "    total_loss += distributed_train_step(batch)\r\n",
        "    num_batches +=1\r\n",
        "  train_loss = total_loss/num_batches\r\n",
        "\r\n",
        "  for batch in test_dist_dataset:\r\n",
        "    distributed_test_step(batch)\r\n",
        "\r\n",
        "  template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \" \"Test Accuracy: {}\")\r\n",
        "  print (template.format(epoch+1, train_loss, train_accuracy.result()*100, test_loss.result(), test_accuracy.result()*100))\r\n",
        "\r\n",
        "  test_loss.reset_states()\r\n",
        "  train_accuracy.reset_states()\r\n",
        "  test_accuracy.reset_states()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(64,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
            "Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(32,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
            "Epoch 1, Loss: 0.33434057235717773, Accuracy: 84.86582946777344, Test Loss: 0.33170926570892334, Test Accuracy: 87.93000030517578\n",
            "Epoch 2, Loss: 0.28858351707458496, Accuracy: 89.30332946777344, Test Loss: 0.303122878074646, Test Accuracy: 88.76000213623047\n",
            "Epoch 3, Loss: 0.2555476725101471, Accuracy: 90.63500213623047, Test Loss: 0.2796458601951599, Test Accuracy: 89.80000305175781\n",
            "Epoch 4, Loss: 0.2331286072731018, Accuracy: 91.34333038330078, Test Loss: 0.2780916392803192, Test Accuracy: 89.81000518798828\n",
            "Epoch 5, Loss: 0.2133655548095703, Accuracy: 92.1816635131836, Test Loss: 0.2587366998195648, Test Accuracy: 90.5999984741211\n",
            "Epoch 6, Loss: 0.19528432190418243, Accuracy: 92.69667053222656, Test Loss: 0.2511894404888153, Test Accuracy: 90.90999603271484\n",
            "Epoch 7, Loss: 0.1809903383255005, Accuracy: 93.21833038330078, Test Loss: 0.25068145990371704, Test Accuracy: 91.18000030517578\n",
            "Epoch 8, Loss: 0.16472187638282776, Accuracy: 93.88500213623047, Test Loss: 0.2511831820011139, Test Accuracy: 91.19999694824219\n",
            "Epoch 9, Loss: 0.1522560566663742, Accuracy: 94.31333923339844, Test Loss: 0.2598930895328522, Test Accuracy: 91.33999633789062\n",
            "Epoch 10, Loss: 0.14051218330860138, Accuracy: 94.75, Test Loss: 0.2710065245628357, Test Accuracy: 90.91999816894531\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}