{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regression_and_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMjERRLFRi8q5HmXqxhRYC+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salim-hbk/ai-ml/blob/main/Regression_and_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cwqdwRYdE4V"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBN_2TEzdWHB"
      },
      "source": [
        "zip_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00320/student.zip'\r\n",
        "\r\n",
        "import urllib\r\n",
        "import zipfile\r\n",
        "\r\n",
        "url = zip_url\r\n",
        "extract_dir = \"sample_data\"\r\n",
        "\r\n",
        "zip_path, _ = urllib.request.urlretrieve(url)\r\n",
        "with zipfile.ZipFile(zip_path, \"r\") as f:\r\n",
        "    f.extractall(extract_dir)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "IVY6ZmeDf5kM",
        "outputId": "9700a5e8-a9d3-440f-a9c0-e1283db2f9b3"
      },
      "source": [
        "df = pd.read_csv('./sample_data/student-mat.csv',sep=';')\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>school</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>address</th>\n",
              "      <th>famsize</th>\n",
              "      <th>Pstatus</th>\n",
              "      <th>Medu</th>\n",
              "      <th>Fedu</th>\n",
              "      <th>Mjob</th>\n",
              "      <th>Fjob</th>\n",
              "      <th>reason</th>\n",
              "      <th>guardian</th>\n",
              "      <th>traveltime</th>\n",
              "      <th>studytime</th>\n",
              "      <th>failures</th>\n",
              "      <th>schoolsup</th>\n",
              "      <th>famsup</th>\n",
              "      <th>paid</th>\n",
              "      <th>activities</th>\n",
              "      <th>nursery</th>\n",
              "      <th>higher</th>\n",
              "      <th>internet</th>\n",
              "      <th>romantic</th>\n",
              "      <th>famrel</th>\n",
              "      <th>freetime</th>\n",
              "      <th>goout</th>\n",
              "      <th>Dalc</th>\n",
              "      <th>Walc</th>\n",
              "      <th>health</th>\n",
              "      <th>absences</th>\n",
              "      <th>G1</th>\n",
              "      <th>G2</th>\n",
              "      <th>G3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GP</td>\n",
              "      <td>F</td>\n",
              "      <td>18</td>\n",
              "      <td>U</td>\n",
              "      <td>GT3</td>\n",
              "      <td>A</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>at_home</td>\n",
              "      <td>teacher</td>\n",
              "      <td>course</td>\n",
              "      <td>mother</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GP</td>\n",
              "      <td>F</td>\n",
              "      <td>17</td>\n",
              "      <td>U</td>\n",
              "      <td>GT3</td>\n",
              "      <td>T</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>at_home</td>\n",
              "      <td>other</td>\n",
              "      <td>course</td>\n",
              "      <td>father</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GP</td>\n",
              "      <td>F</td>\n",
              "      <td>15</td>\n",
              "      <td>U</td>\n",
              "      <td>LE3</td>\n",
              "      <td>T</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>at_home</td>\n",
              "      <td>other</td>\n",
              "      <td>other</td>\n",
              "      <td>mother</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>GP</td>\n",
              "      <td>F</td>\n",
              "      <td>15</td>\n",
              "      <td>U</td>\n",
              "      <td>GT3</td>\n",
              "      <td>T</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>health</td>\n",
              "      <td>services</td>\n",
              "      <td>home</td>\n",
              "      <td>mother</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>14</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>GP</td>\n",
              "      <td>F</td>\n",
              "      <td>16</td>\n",
              "      <td>U</td>\n",
              "      <td>GT3</td>\n",
              "      <td>T</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>other</td>\n",
              "      <td>other</td>\n",
              "      <td>home</td>\n",
              "      <td>father</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  school sex  age address famsize Pstatus  ...  Walc  health absences  G1  G2  G3\n",
              "0     GP   F   18       U     GT3       A  ...     1       3        6   5   6   6\n",
              "1     GP   F   17       U     GT3       T  ...     1       3        4   5   5   6\n",
              "2     GP   F   15       U     LE3       T  ...     3       3       10   7   8  10\n",
              "3     GP   F   15       U     GT3       T  ...     1       5        2  15  14  15\n",
              "4     GP   F   16       U     GT3       T  ...     2       5        4   6  10  10\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "RFEIOwIme5ck",
        "outputId": "3489ccda-35a1-41bb-c915-0afaad62cea3"
      },
      "source": [
        "df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data', names=['Sex', 'Length', 'Diameter', 'Height', 'WholeWeight', 'ShuckedWeight', 'VisceraWeight', 'ShellWeight', 'Rings'])\r\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sex</th>\n",
              "      <th>Length</th>\n",
              "      <th>Diameter</th>\n",
              "      <th>Height</th>\n",
              "      <th>WholeWeight</th>\n",
              "      <th>ShuckedWeight</th>\n",
              "      <th>VisceraWeight</th>\n",
              "      <th>ShellWeight</th>\n",
              "      <th>Rings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>M</td>\n",
              "      <td>0.455</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.5140</td>\n",
              "      <td>0.2245</td>\n",
              "      <td>0.1010</td>\n",
              "      <td>0.150</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.2255</td>\n",
              "      <td>0.0995</td>\n",
              "      <td>0.0485</td>\n",
              "      <td>0.070</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>F</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.6770</td>\n",
              "      <td>0.2565</td>\n",
              "      <td>0.1415</td>\n",
              "      <td>0.210</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>M</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.5160</td>\n",
              "      <td>0.2155</td>\n",
              "      <td>0.1140</td>\n",
              "      <td>0.155</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I</td>\n",
              "      <td>0.330</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.0895</td>\n",
              "      <td>0.0395</td>\n",
              "      <td>0.055</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Sex  Length  Diameter  ...  VisceraWeight  ShellWeight  Rings\n",
              "0   M   0.455     0.365  ...         0.1010        0.150     15\n",
              "1   M   0.350     0.265  ...         0.0485        0.070      7\n",
              "2   F   0.530     0.420  ...         0.1415        0.210      9\n",
              "3   M   0.440     0.365  ...         0.1140        0.155     10\n",
              "4   I   0.330     0.255  ...         0.0395        0.055      7\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "nOVjmzgLgqSB",
        "outputId": "6e752a5d-2922-4ddd-fad4-a3256b5511fe"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Length</th>\n",
              "      <th>Diameter</th>\n",
              "      <th>Height</th>\n",
              "      <th>WholeWeight</th>\n",
              "      <th>ShuckedWeight</th>\n",
              "      <th>VisceraWeight</th>\n",
              "      <th>ShellWeight</th>\n",
              "      <th>Rings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4177.000000</td>\n",
              "      <td>4177.000000</td>\n",
              "      <td>4177.000000</td>\n",
              "      <td>4177.000000</td>\n",
              "      <td>4177.000000</td>\n",
              "      <td>4177.000000</td>\n",
              "      <td>4177.000000</td>\n",
              "      <td>4177.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.523992</td>\n",
              "      <td>0.407881</td>\n",
              "      <td>0.139516</td>\n",
              "      <td>0.828742</td>\n",
              "      <td>0.359367</td>\n",
              "      <td>0.180594</td>\n",
              "      <td>0.238831</td>\n",
              "      <td>9.933684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.120093</td>\n",
              "      <td>0.099240</td>\n",
              "      <td>0.041827</td>\n",
              "      <td>0.490389</td>\n",
              "      <td>0.221963</td>\n",
              "      <td>0.109614</td>\n",
              "      <td>0.139203</td>\n",
              "      <td>3.224169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.055000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.350000</td>\n",
              "      <td>0.115000</td>\n",
              "      <td>0.441500</td>\n",
              "      <td>0.186000</td>\n",
              "      <td>0.093500</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>8.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.545000</td>\n",
              "      <td>0.425000</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.799500</td>\n",
              "      <td>0.336000</td>\n",
              "      <td>0.171000</td>\n",
              "      <td>0.234000</td>\n",
              "      <td>9.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.615000</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>0.165000</td>\n",
              "      <td>1.153000</td>\n",
              "      <td>0.502000</td>\n",
              "      <td>0.253000</td>\n",
              "      <td>0.329000</td>\n",
              "      <td>11.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.815000</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>1.130000</td>\n",
              "      <td>2.825500</td>\n",
              "      <td>1.488000</td>\n",
              "      <td>0.760000</td>\n",
              "      <td>1.005000</td>\n",
              "      <td>29.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Length     Diameter  ...  ShellWeight        Rings\n",
              "count  4177.000000  4177.000000  ...  4177.000000  4177.000000\n",
              "mean      0.523992     0.407881  ...     0.238831     9.933684\n",
              "std       0.120093     0.099240  ...     0.139203     3.224169\n",
              "min       0.075000     0.055000  ...     0.001500     1.000000\n",
              "25%       0.450000     0.350000  ...     0.130000     8.000000\n",
              "50%       0.545000     0.425000  ...     0.234000     9.000000\n",
              "75%       0.615000     0.480000  ...     0.329000    11.000000\n",
              "max       0.815000     0.650000  ...     1.005000    29.000000\n",
              "\n",
              "[8 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCIfamVsg_y8"
      },
      "source": [
        "data = df.copy()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbVVCVdHi1BG"
      },
      "source": [
        "#dummy_vals = pd.get_dummies(data['Sex'])\r\n",
        "data['Sex'] = data['Sex'].map({'M':0, 'F':1, 'I':2})"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSr7yGT-jQ5H"
      },
      "source": [
        "#dummy_vals.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "HcfKxF_Ujccr",
        "outputId": "91479822-8e8a-4e85-bad8-ddef332b7eb0"
      },
      "source": [
        "#new_data = pd.concat([data.reset_index(drop=True), dummy_vals.reset_index(drop=True)], axis=1)\r\n",
        "data.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sex</th>\n",
              "      <th>Length</th>\n",
              "      <th>Diameter</th>\n",
              "      <th>Height</th>\n",
              "      <th>WholeWeight</th>\n",
              "      <th>ShuckedWeight</th>\n",
              "      <th>VisceraWeight</th>\n",
              "      <th>ShellWeight</th>\n",
              "      <th>Rings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.455</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.5140</td>\n",
              "      <td>0.2245</td>\n",
              "      <td>0.1010</td>\n",
              "      <td>0.150</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.2255</td>\n",
              "      <td>0.0995</td>\n",
              "      <td>0.0485</td>\n",
              "      <td>0.070</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.6770</td>\n",
              "      <td>0.2565</td>\n",
              "      <td>0.1415</td>\n",
              "      <td>0.210</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.5160</td>\n",
              "      <td>0.2155</td>\n",
              "      <td>0.1140</td>\n",
              "      <td>0.155</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>0.330</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.0895</td>\n",
              "      <td>0.0395</td>\n",
              "      <td>0.055</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sex  Length  Diameter  ...  VisceraWeight  ShellWeight  Rings\n",
              "0    0   0.455     0.365  ...         0.1010        0.150     15\n",
              "1    0   0.350     0.265  ...         0.0485        0.070      7\n",
              "2    1   0.530     0.420  ...         0.1415        0.210      9\n",
              "3    0   0.440     0.365  ...         0.1140        0.155     10\n",
              "4    2   0.330     0.255  ...         0.0395        0.055      7\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXq6JVG6qG5J"
      },
      "source": [
        "#final_data = new_data.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYu7jyVzqNXd"
      },
      "source": [
        "#final_data.drop('Sex', axis=1, inplace=True)\r\n",
        "#final_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "1Nzwa6tRwTTX",
        "outputId": "76d942c6-aeac-4028-fb1a-b3a077c39ce0"
      },
      "source": [
        "data['Rings'].hist()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb6ce2974e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "9x-uK2u3wZpK",
        "outputId": "71b81337-c3de-49c1-fae3-ed5f669a6c43"
      },
      "source": [
        "data = data[((data['Rings']>8) & (data['Rings']<12))]\r\n",
        "data = data.reset_index(drop=True)\r\n",
        "data['Rings'].hist()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb67bf34198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGyhdyLL1XHF"
      },
      "source": [
        "def format_output(data):\r\n",
        "  Sex = data.pop('Sex')\r\n",
        "  Sex = np.array(Sex)\r\n",
        "  Rings = data.pop('Rings')\r\n",
        "  Rings = np.array(Rings)\r\n",
        "  return (Rings, Sex)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vU6sXC8xREG"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "train, test = train_test_split(data, test_size=0.2, random_state = 1)\r\n",
        "train_x =  train[train.columns[1:-1]]\r\n",
        "test_x = test[test.columns[1:-1]]\r\n",
        "\r\n",
        "train_y = format_output(train)\r\n",
        "test_y  = format_output(test)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyEMKPf8qV5s",
        "outputId": "779896ca-8a57-4e2e-98b1-1cd7ff216de2"
      },
      "source": [
        "print(train_x.count())\r\n",
        "print(test_x.count())"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length           1448\n",
            "Diameter         1448\n",
            "Height           1448\n",
            "WholeWeight      1448\n",
            "ShuckedWeight    1448\n",
            "VisceraWeight    1448\n",
            "ShellWeight      1448\n",
            "dtype: int64\n",
            "Length           362\n",
            "Diameter         362\n",
            "Height           362\n",
            "WholeWeight      362\n",
            "ShuckedWeight    362\n",
            "VisceraWeight    362\n",
            "ShellWeight      362\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "tfRYsVbHuHZs",
        "outputId": "c47d42f1-abfe-4e1a-dff5-a0aec6201a17"
      },
      "source": [
        "train_stats = train_x.describe()\r\n",
        "train_stats = train_stats.transpose()\r\n",
        "train_stats"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Length</th>\n",
              "      <td>1448.0</td>\n",
              "      <td>0.570294</td>\n",
              "      <td>0.088915</td>\n",
              "      <td>0.2800</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.58000</td>\n",
              "      <td>0.63000</td>\n",
              "      <td>0.7800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Diameter</th>\n",
              "      <td>1448.0</td>\n",
              "      <td>0.445283</td>\n",
              "      <td>0.072691</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.45500</td>\n",
              "      <td>0.49500</td>\n",
              "      <td>0.6300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Height</th>\n",
              "      <td>1448.0</td>\n",
              "      <td>0.151733</td>\n",
              "      <td>0.030418</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.15000</td>\n",
              "      <td>0.17000</td>\n",
              "      <td>0.5150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WholeWeight</th>\n",
              "      <td>1448.0</td>\n",
              "      <td>0.984152</td>\n",
              "      <td>0.428898</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.685500</td>\n",
              "      <td>0.97025</td>\n",
              "      <td>1.25425</td>\n",
              "      <td>2.6570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ShuckedWeight</th>\n",
              "      <td>1448.0</td>\n",
              "      <td>0.440198</td>\n",
              "      <td>0.209522</td>\n",
              "      <td>0.0415</td>\n",
              "      <td>0.289875</td>\n",
              "      <td>0.43750</td>\n",
              "      <td>0.57075</td>\n",
              "      <td>1.4880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VisceraWeight</th>\n",
              "      <td>1448.0</td>\n",
              "      <td>0.217438</td>\n",
              "      <td>0.098625</td>\n",
              "      <td>0.0240</td>\n",
              "      <td>0.146500</td>\n",
              "      <td>0.21025</td>\n",
              "      <td>0.28150</td>\n",
              "      <td>0.5745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ShellWeight</th>\n",
              "      <td>1448.0</td>\n",
              "      <td>0.274192</td>\n",
              "      <td>0.109706</td>\n",
              "      <td>0.0400</td>\n",
              "      <td>0.195000</td>\n",
              "      <td>0.27425</td>\n",
              "      <td>0.34400</td>\n",
              "      <td>0.6745</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                count      mean       std  ...      50%      75%     max\n",
              "Length         1448.0  0.570294  0.088915  ...  0.58000  0.63000  0.7800\n",
              "Diameter       1448.0  0.445283  0.072691  ...  0.45500  0.49500  0.6300\n",
              "Height         1448.0  0.151733  0.030418  ...  0.15000  0.17000  0.5150\n",
              "WholeWeight    1448.0  0.984152  0.428898  ...  0.97025  1.25425  2.6570\n",
              "ShuckedWeight  1448.0  0.440198  0.209522  ...  0.43750  0.57075  1.4880\n",
              "VisceraWeight  1448.0  0.217438  0.098625  ...  0.21025  0.28150  0.5745\n",
              "ShellWeight    1448.0  0.274192  0.109706  ...  0.27425  0.34400  0.6745\n",
              "\n",
              "[7 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PABnjLuDzb6k"
      },
      "source": [
        "def norm(data):\r\n",
        "  return (data - train_stats['mean'])/train_stats['std']"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGD_22mq2i-j"
      },
      "source": [
        "norm_train_x = norm(train_x)\r\n",
        "norm_test_x = norm(test_x)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uuvtWv77V93"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7q5cQ7J20mz"
      },
      "source": [
        "def base_net(input):\r\n",
        "  x = tf.keras.layers.Dense(128, activation='linear')(input)\r\n",
        "  return x"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-32-EWAb8AWv"
      },
      "source": [
        "input_layer = tf.keras.layers.Input(shape=[7])\r\n",
        "base_input = base_net(input_layer)\r\n",
        "output_regression = tf.keras.layers.Dense(1, activation='linear', name='output_regression')(base_input)\r\n",
        "output_classification = tf.keras.layers.Dense(3, activation='softmax', name='output_classification' )(base_input)\r\n",
        "model = tf.keras.models.Model(inputs=input_layer, outputs=[output_regression, output_classification])"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY0T_rndBO6Q",
        "outputId": "3cb122e7-e665-4df7-9bdc-4424e4956b76"
      },
      "source": [
        "model.compile(optimizer='sgd', loss={'output_regression': 'mse', 'output_classification': 'sparse_categorical_crossentropy'})\r\n",
        "model.fit(norm_train_x, train_y, epochs=600)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 34.3027 - output_regression_loss: 33.1711 - output_classification_loss: 1.1317\n",
            "Epoch 2/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5992 - output_regression_loss: 0.6128 - output_classification_loss: 0.9863\n",
            "Epoch 3/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5968 - output_regression_loss: 0.6302 - output_classification_loss: 0.9666\n",
            "Epoch 4/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5822 - output_regression_loss: 0.6057 - output_classification_loss: 0.9765\n",
            "Epoch 5/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5816 - output_regression_loss: 0.6264 - output_classification_loss: 0.9552\n",
            "Epoch 6/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5506 - output_regression_loss: 0.6035 - output_classification_loss: 0.9472\n",
            "Epoch 7/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.6319 - output_regression_loss: 0.6634 - output_classification_loss: 0.9685\n",
            "Epoch 8/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5649 - output_regression_loss: 0.6189 - output_classification_loss: 0.9460\n",
            "Epoch 9/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.6000 - output_regression_loss: 0.6449 - output_classification_loss: 0.9551\n",
            "Epoch 10/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5619 - output_regression_loss: 0.6135 - output_classification_loss: 0.9485\n",
            "Epoch 11/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5875 - output_regression_loss: 0.6326 - output_classification_loss: 0.9549\n",
            "Epoch 12/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5287 - output_regression_loss: 0.5878 - output_classification_loss: 0.9409\n",
            "Epoch 13/600\n",
            "46/46 [==============================] - 0s 977us/step - loss: 1.5254 - output_regression_loss: 0.5893 - output_classification_loss: 0.9361\n",
            "Epoch 14/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5460 - output_regression_loss: 0.6182 - output_classification_loss: 0.9278\n",
            "Epoch 15/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5660 - output_regression_loss: 0.6360 - output_classification_loss: 0.9300\n",
            "Epoch 16/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5334 - output_regression_loss: 0.6037 - output_classification_loss: 0.9298\n",
            "Epoch 17/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5320 - output_regression_loss: 0.5962 - output_classification_loss: 0.9358\n",
            "Epoch 18/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5613 - output_regression_loss: 0.6326 - output_classification_loss: 0.9287\n",
            "Epoch 19/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5528 - output_regression_loss: 0.6261 - output_classification_loss: 0.9266\n",
            "Epoch 20/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5540 - output_regression_loss: 0.6185 - output_classification_loss: 0.9355\n",
            "Epoch 21/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4943 - output_regression_loss: 0.5808 - output_classification_loss: 0.9134\n",
            "Epoch 22/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5539 - output_regression_loss: 0.6261 - output_classification_loss: 0.9278\n",
            "Epoch 23/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5547 - output_regression_loss: 0.6236 - output_classification_loss: 0.9311\n",
            "Epoch 24/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4864 - output_regression_loss: 0.5714 - output_classification_loss: 0.9150\n",
            "Epoch 25/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5334 - output_regression_loss: 0.6098 - output_classification_loss: 0.9236\n",
            "Epoch 26/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5777 - output_regression_loss: 0.6600 - output_classification_loss: 0.9177\n",
            "Epoch 27/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4857 - output_regression_loss: 0.5705 - output_classification_loss: 0.9152\n",
            "Epoch 28/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5225 - output_regression_loss: 0.6056 - output_classification_loss: 0.9169\n",
            "Epoch 29/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5410 - output_regression_loss: 0.6117 - output_classification_loss: 0.9293\n",
            "Epoch 30/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5316 - output_regression_loss: 0.6025 - output_classification_loss: 0.9291\n",
            "Epoch 31/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5416 - output_regression_loss: 0.6386 - output_classification_loss: 0.9030\n",
            "Epoch 32/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5186 - output_regression_loss: 0.5959 - output_classification_loss: 0.9227\n",
            "Epoch 33/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5545 - output_regression_loss: 0.6541 - output_classification_loss: 0.9004\n",
            "Epoch 34/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5299 - output_regression_loss: 0.5996 - output_classification_loss: 0.9303\n",
            "Epoch 35/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4988 - output_regression_loss: 0.5960 - output_classification_loss: 0.9028\n",
            "Epoch 36/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5941 - output_regression_loss: 0.6716 - output_classification_loss: 0.9225\n",
            "Epoch 37/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5200 - output_regression_loss: 0.6147 - output_classification_loss: 0.9052\n",
            "Epoch 38/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5047 - output_regression_loss: 0.5969 - output_classification_loss: 0.9079\n",
            "Epoch 39/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5761 - output_regression_loss: 0.6613 - output_classification_loss: 0.9148\n",
            "Epoch 40/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4675 - output_regression_loss: 0.5394 - output_classification_loss: 0.9281\n",
            "Epoch 41/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5133 - output_regression_loss: 0.5980 - output_classification_loss: 0.9153\n",
            "Epoch 42/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5529 - output_regression_loss: 0.6269 - output_classification_loss: 0.9259\n",
            "Epoch 43/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5100 - output_regression_loss: 0.6033 - output_classification_loss: 0.9066\n",
            "Epoch 44/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4903 - output_regression_loss: 0.5840 - output_classification_loss: 0.9063\n",
            "Epoch 45/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5005 - output_regression_loss: 0.5805 - output_classification_loss: 0.9200\n",
            "Epoch 46/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5060 - output_regression_loss: 0.5969 - output_classification_loss: 0.9091\n",
            "Epoch 47/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4831 - output_regression_loss: 0.5737 - output_classification_loss: 0.9094\n",
            "Epoch 48/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5215 - output_regression_loss: 0.6096 - output_classification_loss: 0.9119\n",
            "Epoch 49/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4811 - output_regression_loss: 0.5829 - output_classification_loss: 0.8982\n",
            "Epoch 50/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5282 - output_regression_loss: 0.6086 - output_classification_loss: 0.9197\n",
            "Epoch 51/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4866 - output_regression_loss: 0.5798 - output_classification_loss: 0.9068\n",
            "Epoch 52/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5011 - output_regression_loss: 0.5811 - output_classification_loss: 0.9200\n",
            "Epoch 53/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5376 - output_regression_loss: 0.6189 - output_classification_loss: 0.9186\n",
            "Epoch 54/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5469 - output_regression_loss: 0.6215 - output_classification_loss: 0.9254\n",
            "Epoch 55/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4610 - output_regression_loss: 0.5619 - output_classification_loss: 0.8991\n",
            "Epoch 56/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4912 - output_regression_loss: 0.5704 - output_classification_loss: 0.9208\n",
            "Epoch 57/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5104 - output_regression_loss: 0.6020 - output_classification_loss: 0.9084\n",
            "Epoch 58/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4830 - output_regression_loss: 0.5782 - output_classification_loss: 0.9048\n",
            "Epoch 59/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4916 - output_regression_loss: 0.5813 - output_classification_loss: 0.9103\n",
            "Epoch 60/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4997 - output_regression_loss: 0.5870 - output_classification_loss: 0.9127\n",
            "Epoch 61/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5208 - output_regression_loss: 0.6193 - output_classification_loss: 0.9015\n",
            "Epoch 62/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5138 - output_regression_loss: 0.5959 - output_classification_loss: 0.9179\n",
            "Epoch 63/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4763 - output_regression_loss: 0.5661 - output_classification_loss: 0.9102\n",
            "Epoch 64/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5271 - output_regression_loss: 0.6141 - output_classification_loss: 0.9130\n",
            "Epoch 65/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5052 - output_regression_loss: 0.6006 - output_classification_loss: 0.9046\n",
            "Epoch 66/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4951 - output_regression_loss: 0.5807 - output_classification_loss: 0.9144\n",
            "Epoch 67/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4577 - output_regression_loss: 0.5748 - output_classification_loss: 0.8829\n",
            "Epoch 68/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4740 - output_regression_loss: 0.5726 - output_classification_loss: 0.9014\n",
            "Epoch 69/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5059 - output_regression_loss: 0.5950 - output_classification_loss: 0.9108\n",
            "Epoch 70/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5142 - output_regression_loss: 0.6012 - output_classification_loss: 0.9130\n",
            "Epoch 71/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4838 - output_regression_loss: 0.5689 - output_classification_loss: 0.9149\n",
            "Epoch 72/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5122 - output_regression_loss: 0.6106 - output_classification_loss: 0.9016\n",
            "Epoch 73/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4628 - output_regression_loss: 0.5755 - output_classification_loss: 0.8873\n",
            "Epoch 74/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4731 - output_regression_loss: 0.5786 - output_classification_loss: 0.8945\n",
            "Epoch 75/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4807 - output_regression_loss: 0.5791 - output_classification_loss: 0.9016\n",
            "Epoch 76/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4840 - output_regression_loss: 0.5859 - output_classification_loss: 0.8981\n",
            "Epoch 77/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4919 - output_regression_loss: 0.5804 - output_classification_loss: 0.9115\n",
            "Epoch 78/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5097 - output_regression_loss: 0.6065 - output_classification_loss: 0.9032\n",
            "Epoch 79/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4937 - output_regression_loss: 0.5952 - output_classification_loss: 0.8984\n",
            "Epoch 80/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4871 - output_regression_loss: 0.5958 - output_classification_loss: 0.8914\n",
            "Epoch 81/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4738 - output_regression_loss: 0.5670 - output_classification_loss: 0.9068\n",
            "Epoch 82/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4685 - output_regression_loss: 0.5596 - output_classification_loss: 0.9090\n",
            "Epoch 83/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4783 - output_regression_loss: 0.5829 - output_classification_loss: 0.8953\n",
            "Epoch 84/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4800 - output_regression_loss: 0.5825 - output_classification_loss: 0.8976\n",
            "Epoch 85/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4996 - output_regression_loss: 0.5792 - output_classification_loss: 0.9205\n",
            "Epoch 86/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4658 - output_regression_loss: 0.5704 - output_classification_loss: 0.8954\n",
            "Epoch 87/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4601 - output_regression_loss: 0.5604 - output_classification_loss: 0.8996\n",
            "Epoch 88/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5132 - output_regression_loss: 0.6126 - output_classification_loss: 0.9007\n",
            "Epoch 89/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4933 - output_regression_loss: 0.5886 - output_classification_loss: 0.9047\n",
            "Epoch 90/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5262 - output_regression_loss: 0.6156 - output_classification_loss: 0.9106\n",
            "Epoch 91/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4728 - output_regression_loss: 0.5699 - output_classification_loss: 0.9029\n",
            "Epoch 92/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4499 - output_regression_loss: 0.5507 - output_classification_loss: 0.8992\n",
            "Epoch 93/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5094 - output_regression_loss: 0.5979 - output_classification_loss: 0.9115\n",
            "Epoch 94/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5293 - output_regression_loss: 0.6076 - output_classification_loss: 0.9217\n",
            "Epoch 95/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5112 - output_regression_loss: 0.6065 - output_classification_loss: 0.9047\n",
            "Epoch 96/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4725 - output_regression_loss: 0.5775 - output_classification_loss: 0.8950\n",
            "Epoch 97/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4753 - output_regression_loss: 0.5742 - output_classification_loss: 0.9010\n",
            "Epoch 98/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5316 - output_regression_loss: 0.6319 - output_classification_loss: 0.8996\n",
            "Epoch 99/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5050 - output_regression_loss: 0.6048 - output_classification_loss: 0.9002\n",
            "Epoch 100/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4966 - output_regression_loss: 0.5780 - output_classification_loss: 0.9186\n",
            "Epoch 101/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4667 - output_regression_loss: 0.5745 - output_classification_loss: 0.8922\n",
            "Epoch 102/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4866 - output_regression_loss: 0.5737 - output_classification_loss: 0.9130\n",
            "Epoch 103/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4734 - output_regression_loss: 0.5808 - output_classification_loss: 0.8927\n",
            "Epoch 104/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4847 - output_regression_loss: 0.5770 - output_classification_loss: 0.9077\n",
            "Epoch 105/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4955 - output_regression_loss: 0.5889 - output_classification_loss: 0.9066\n",
            "Epoch 106/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5170 - output_regression_loss: 0.5914 - output_classification_loss: 0.9256\n",
            "Epoch 107/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5126 - output_regression_loss: 0.5967 - output_classification_loss: 0.9159\n",
            "Epoch 108/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4917 - output_regression_loss: 0.5905 - output_classification_loss: 0.9012\n",
            "Epoch 109/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4731 - output_regression_loss: 0.5724 - output_classification_loss: 0.9008\n",
            "Epoch 110/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5284 - output_regression_loss: 0.6276 - output_classification_loss: 0.9009\n",
            "Epoch 111/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4816 - output_regression_loss: 0.5842 - output_classification_loss: 0.8974\n",
            "Epoch 112/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4763 - output_regression_loss: 0.5795 - output_classification_loss: 0.8968\n",
            "Epoch 113/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4980 - output_regression_loss: 0.5968 - output_classification_loss: 0.9011\n",
            "Epoch 114/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5458 - output_regression_loss: 0.6294 - output_classification_loss: 0.9163\n",
            "Epoch 115/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4668 - output_regression_loss: 0.5602 - output_classification_loss: 0.9066\n",
            "Epoch 116/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5095 - output_regression_loss: 0.5985 - output_classification_loss: 0.9110\n",
            "Epoch 117/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4903 - output_regression_loss: 0.5816 - output_classification_loss: 0.9088\n",
            "Epoch 118/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4945 - output_regression_loss: 0.5865 - output_classification_loss: 0.9080\n",
            "Epoch 119/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4622 - output_regression_loss: 0.5775 - output_classification_loss: 0.8847\n",
            "Epoch 120/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4699 - output_regression_loss: 0.5665 - output_classification_loss: 0.9034\n",
            "Epoch 121/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4726 - output_regression_loss: 0.5639 - output_classification_loss: 0.9086\n",
            "Epoch 122/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5153 - output_regression_loss: 0.5859 - output_classification_loss: 0.9295\n",
            "Epoch 123/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5041 - output_regression_loss: 0.5929 - output_classification_loss: 0.9112\n",
            "Epoch 124/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4792 - output_regression_loss: 0.5883 - output_classification_loss: 0.8909\n",
            "Epoch 125/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5250 - output_regression_loss: 0.6068 - output_classification_loss: 0.9182\n",
            "Epoch 126/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4602 - output_regression_loss: 0.5664 - output_classification_loss: 0.8938\n",
            "Epoch 127/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4904 - output_regression_loss: 0.5826 - output_classification_loss: 0.9077\n",
            "Epoch 128/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5032 - output_regression_loss: 0.5912 - output_classification_loss: 0.9120\n",
            "Epoch 129/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4902 - output_regression_loss: 0.5786 - output_classification_loss: 0.9116\n",
            "Epoch 130/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5030 - output_regression_loss: 0.5949 - output_classification_loss: 0.9081\n",
            "Epoch 131/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4618 - output_regression_loss: 0.5756 - output_classification_loss: 0.8862\n",
            "Epoch 132/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4533 - output_regression_loss: 0.5706 - output_classification_loss: 0.8827\n",
            "Epoch 133/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4845 - output_regression_loss: 0.5882 - output_classification_loss: 0.8963\n",
            "Epoch 134/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4808 - output_regression_loss: 0.5722 - output_classification_loss: 0.9087\n",
            "Epoch 135/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4929 - output_regression_loss: 0.5866 - output_classification_loss: 0.9062\n",
            "Epoch 136/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4405 - output_regression_loss: 0.5522 - output_classification_loss: 0.8884\n",
            "Epoch 137/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5268 - output_regression_loss: 0.6300 - output_classification_loss: 0.8968\n",
            "Epoch 138/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5107 - output_regression_loss: 0.6131 - output_classification_loss: 0.8976\n",
            "Epoch 139/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4993 - output_regression_loss: 0.6034 - output_classification_loss: 0.8959\n",
            "Epoch 140/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4980 - output_regression_loss: 0.5997 - output_classification_loss: 0.8982\n",
            "Epoch 141/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5209 - output_regression_loss: 0.6044 - output_classification_loss: 0.9165\n",
            "Epoch 142/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4602 - output_regression_loss: 0.5665 - output_classification_loss: 0.8938\n",
            "Epoch 143/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5211 - output_regression_loss: 0.5961 - output_classification_loss: 0.9251\n",
            "Epoch 144/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4677 - output_regression_loss: 0.5674 - output_classification_loss: 0.9003\n",
            "Epoch 145/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4855 - output_regression_loss: 0.5987 - output_classification_loss: 0.8868\n",
            "Epoch 146/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5290 - output_regression_loss: 0.6107 - output_classification_loss: 0.9182\n",
            "Epoch 147/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4872 - output_regression_loss: 0.5745 - output_classification_loss: 0.9127\n",
            "Epoch 148/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4590 - output_regression_loss: 0.5653 - output_classification_loss: 0.8937\n",
            "Epoch 149/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4599 - output_regression_loss: 0.5549 - output_classification_loss: 0.9050\n",
            "Epoch 150/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4949 - output_regression_loss: 0.5723 - output_classification_loss: 0.9225\n",
            "Epoch 151/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5038 - output_regression_loss: 0.5871 - output_classification_loss: 0.9167\n",
            "Epoch 152/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4836 - output_regression_loss: 0.5735 - output_classification_loss: 0.9101\n",
            "Epoch 153/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4782 - output_regression_loss: 0.5703 - output_classification_loss: 0.9079\n",
            "Epoch 154/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4914 - output_regression_loss: 0.5917 - output_classification_loss: 0.8997\n",
            "Epoch 155/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4640 - output_regression_loss: 0.5686 - output_classification_loss: 0.8953\n",
            "Epoch 156/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5086 - output_regression_loss: 0.5837 - output_classification_loss: 0.9249\n",
            "Epoch 157/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5022 - output_regression_loss: 0.5891 - output_classification_loss: 0.9131\n",
            "Epoch 158/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4691 - output_regression_loss: 0.5785 - output_classification_loss: 0.8906\n",
            "Epoch 159/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5042 - output_regression_loss: 0.5840 - output_classification_loss: 0.9202\n",
            "Epoch 160/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5224 - output_regression_loss: 0.6021 - output_classification_loss: 0.9202\n",
            "Epoch 161/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4840 - output_regression_loss: 0.5742 - output_classification_loss: 0.9099\n",
            "Epoch 162/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4625 - output_regression_loss: 0.5403 - output_classification_loss: 0.9222\n",
            "Epoch 163/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5101 - output_regression_loss: 0.5884 - output_classification_loss: 0.9217\n",
            "Epoch 164/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5107 - output_regression_loss: 0.6013 - output_classification_loss: 0.9094\n",
            "Epoch 165/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4930 - output_regression_loss: 0.5866 - output_classification_loss: 0.9065\n",
            "Epoch 166/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4864 - output_regression_loss: 0.5781 - output_classification_loss: 0.9084\n",
            "Epoch 167/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5080 - output_regression_loss: 0.5981 - output_classification_loss: 0.9099\n",
            "Epoch 168/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4786 - output_regression_loss: 0.5799 - output_classification_loss: 0.8987\n",
            "Epoch 169/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5257 - output_regression_loss: 0.6218 - output_classification_loss: 0.9039\n",
            "Epoch 170/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5077 - output_regression_loss: 0.5886 - output_classification_loss: 0.9190\n",
            "Epoch 171/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4869 - output_regression_loss: 0.5799 - output_classification_loss: 0.9070\n",
            "Epoch 172/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4824 - output_regression_loss: 0.5770 - output_classification_loss: 0.9054\n",
            "Epoch 173/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4571 - output_regression_loss: 0.5569 - output_classification_loss: 0.9002\n",
            "Epoch 174/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5130 - output_regression_loss: 0.6004 - output_classification_loss: 0.9126\n",
            "Epoch 175/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4794 - output_regression_loss: 0.5601 - output_classification_loss: 0.9193\n",
            "Epoch 176/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4876 - output_regression_loss: 0.5843 - output_classification_loss: 0.9032\n",
            "Epoch 177/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4728 - output_regression_loss: 0.5791 - output_classification_loss: 0.8937\n",
            "Epoch 178/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4788 - output_regression_loss: 0.5742 - output_classification_loss: 0.9046\n",
            "Epoch 179/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4635 - output_regression_loss: 0.5689 - output_classification_loss: 0.8946\n",
            "Epoch 180/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4950 - output_regression_loss: 0.5892 - output_classification_loss: 0.9058\n",
            "Epoch 181/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4652 - output_regression_loss: 0.5648 - output_classification_loss: 0.9003\n",
            "Epoch 182/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4790 - output_regression_loss: 0.5768 - output_classification_loss: 0.9022\n",
            "Epoch 183/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5186 - output_regression_loss: 0.6044 - output_classification_loss: 0.9142\n",
            "Epoch 184/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4631 - output_regression_loss: 0.5662 - output_classification_loss: 0.8969\n",
            "Epoch 185/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4732 - output_regression_loss: 0.5737 - output_classification_loss: 0.8995\n",
            "Epoch 186/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4892 - output_regression_loss: 0.5852 - output_classification_loss: 0.9041\n",
            "Epoch 187/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4987 - output_regression_loss: 0.5985 - output_classification_loss: 0.9002\n",
            "Epoch 188/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4491 - output_regression_loss: 0.5593 - output_classification_loss: 0.8898\n",
            "Epoch 189/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4832 - output_regression_loss: 0.5911 - output_classification_loss: 0.8921\n",
            "Epoch 190/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5110 - output_regression_loss: 0.6062 - output_classification_loss: 0.9048\n",
            "Epoch 191/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4881 - output_regression_loss: 0.5862 - output_classification_loss: 0.9019\n",
            "Epoch 192/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4867 - output_regression_loss: 0.5909 - output_classification_loss: 0.8958\n",
            "Epoch 193/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4987 - output_regression_loss: 0.5699 - output_classification_loss: 0.9288\n",
            "Epoch 194/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4825 - output_regression_loss: 0.5873 - output_classification_loss: 0.8951\n",
            "Epoch 195/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5448 - output_regression_loss: 0.6211 - output_classification_loss: 0.9237\n",
            "Epoch 196/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4843 - output_regression_loss: 0.5850 - output_classification_loss: 0.8993\n",
            "Epoch 197/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5103 - output_regression_loss: 0.5873 - output_classification_loss: 0.9230\n",
            "Epoch 198/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4808 - output_regression_loss: 0.5901 - output_classification_loss: 0.8907\n",
            "Epoch 199/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5110 - output_regression_loss: 0.6084 - output_classification_loss: 0.9026\n",
            "Epoch 200/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4786 - output_regression_loss: 0.5628 - output_classification_loss: 0.9158\n",
            "Epoch 201/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5193 - output_regression_loss: 0.6122 - output_classification_loss: 0.9071\n",
            "Epoch 202/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5004 - output_regression_loss: 0.5848 - output_classification_loss: 0.9156\n",
            "Epoch 203/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4759 - output_regression_loss: 0.5755 - output_classification_loss: 0.9004\n",
            "Epoch 204/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4817 - output_regression_loss: 0.5836 - output_classification_loss: 0.8982\n",
            "Epoch 205/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5037 - output_regression_loss: 0.6040 - output_classification_loss: 0.8996\n",
            "Epoch 206/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4778 - output_regression_loss: 0.5860 - output_classification_loss: 0.8917\n",
            "Epoch 207/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4846 - output_regression_loss: 0.5771 - output_classification_loss: 0.9075\n",
            "Epoch 208/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4995 - output_regression_loss: 0.5911 - output_classification_loss: 0.9083\n",
            "Epoch 209/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4795 - output_regression_loss: 0.5748 - output_classification_loss: 0.9048\n",
            "Epoch 210/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4591 - output_regression_loss: 0.5667 - output_classification_loss: 0.8925\n",
            "Epoch 211/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5011 - output_regression_loss: 0.5946 - output_classification_loss: 0.9065\n",
            "Epoch 212/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5182 - output_regression_loss: 0.5989 - output_classification_loss: 0.9192\n",
            "Epoch 213/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4949 - output_regression_loss: 0.5819 - output_classification_loss: 0.9130\n",
            "Epoch 214/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5128 - output_regression_loss: 0.6084 - output_classification_loss: 0.9043\n",
            "Epoch 215/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5090 - output_regression_loss: 0.5953 - output_classification_loss: 0.9137\n",
            "Epoch 216/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4681 - output_regression_loss: 0.5742 - output_classification_loss: 0.8940\n",
            "Epoch 217/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4838 - output_regression_loss: 0.5900 - output_classification_loss: 0.8938\n",
            "Epoch 218/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5149 - output_regression_loss: 0.6193 - output_classification_loss: 0.8956\n",
            "Epoch 219/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4853 - output_regression_loss: 0.5724 - output_classification_loss: 0.9129\n",
            "Epoch 220/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5305 - output_regression_loss: 0.6069 - output_classification_loss: 0.9236\n",
            "Epoch 221/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4840 - output_regression_loss: 0.5819 - output_classification_loss: 0.9021\n",
            "Epoch 222/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4984 - output_regression_loss: 0.5920 - output_classification_loss: 0.9064\n",
            "Epoch 223/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4734 - output_regression_loss: 0.5637 - output_classification_loss: 0.9098\n",
            "Epoch 224/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5259 - output_regression_loss: 0.6308 - output_classification_loss: 0.8951\n",
            "Epoch 225/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4771 - output_regression_loss: 0.5706 - output_classification_loss: 0.9065\n",
            "Epoch 226/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4956 - output_regression_loss: 0.5803 - output_classification_loss: 0.9153\n",
            "Epoch 227/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4854 - output_regression_loss: 0.5662 - output_classification_loss: 0.9192\n",
            "Epoch 228/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4580 - output_regression_loss: 0.5673 - output_classification_loss: 0.8906\n",
            "Epoch 229/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5106 - output_regression_loss: 0.5951 - output_classification_loss: 0.9156\n",
            "Epoch 230/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4744 - output_regression_loss: 0.5779 - output_classification_loss: 0.8965\n",
            "Epoch 231/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4541 - output_regression_loss: 0.5658 - output_classification_loss: 0.8883\n",
            "Epoch 232/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4596 - output_regression_loss: 0.5653 - output_classification_loss: 0.8943\n",
            "Epoch 233/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4779 - output_regression_loss: 0.5607 - output_classification_loss: 0.9171\n",
            "Epoch 234/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5001 - output_regression_loss: 0.5986 - output_classification_loss: 0.9014\n",
            "Epoch 235/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4745 - output_regression_loss: 0.5679 - output_classification_loss: 0.9066\n",
            "Epoch 236/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4858 - output_regression_loss: 0.5846 - output_classification_loss: 0.9012\n",
            "Epoch 237/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4668 - output_regression_loss: 0.5712 - output_classification_loss: 0.8955\n",
            "Epoch 238/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4812 - output_regression_loss: 0.5757 - output_classification_loss: 0.9055\n",
            "Epoch 239/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4943 - output_regression_loss: 0.5857 - output_classification_loss: 0.9086\n",
            "Epoch 240/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4725 - output_regression_loss: 0.5711 - output_classification_loss: 0.9014\n",
            "Epoch 241/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5067 - output_regression_loss: 0.6038 - output_classification_loss: 0.9030\n",
            "Epoch 242/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4587 - output_regression_loss: 0.5497 - output_classification_loss: 0.9090\n",
            "Epoch 243/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4956 - output_regression_loss: 0.5976 - output_classification_loss: 0.8980\n",
            "Epoch 244/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4926 - output_regression_loss: 0.5810 - output_classification_loss: 0.9116\n",
            "Epoch 245/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4795 - output_regression_loss: 0.5692 - output_classification_loss: 0.9103\n",
            "Epoch 246/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4474 - output_regression_loss: 0.5520 - output_classification_loss: 0.8954\n",
            "Epoch 247/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4641 - output_regression_loss: 0.5667 - output_classification_loss: 0.8974\n",
            "Epoch 248/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4845 - output_regression_loss: 0.5782 - output_classification_loss: 0.9063\n",
            "Epoch 249/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4763 - output_regression_loss: 0.5867 - output_classification_loss: 0.8897\n",
            "Epoch 250/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4692 - output_regression_loss: 0.5646 - output_classification_loss: 0.9046\n",
            "Epoch 251/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4596 - output_regression_loss: 0.5597 - output_classification_loss: 0.8999\n",
            "Epoch 252/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4676 - output_regression_loss: 0.5813 - output_classification_loss: 0.8864\n",
            "Epoch 253/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4675 - output_regression_loss: 0.5703 - output_classification_loss: 0.8973\n",
            "Epoch 254/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4601 - output_regression_loss: 0.5632 - output_classification_loss: 0.8969\n",
            "Epoch 255/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4521 - output_regression_loss: 0.5523 - output_classification_loss: 0.8998\n",
            "Epoch 256/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4893 - output_regression_loss: 0.5924 - output_classification_loss: 0.8969\n",
            "Epoch 257/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4788 - output_regression_loss: 0.5807 - output_classification_loss: 0.8981\n",
            "Epoch 258/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4850 - output_regression_loss: 0.5912 - output_classification_loss: 0.8939\n",
            "Epoch 259/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4451 - output_regression_loss: 0.5585 - output_classification_loss: 0.8867\n",
            "Epoch 260/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4647 - output_regression_loss: 0.5599 - output_classification_loss: 0.9048\n",
            "Epoch 261/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4724 - output_regression_loss: 0.5739 - output_classification_loss: 0.8985\n",
            "Epoch 262/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4762 - output_regression_loss: 0.5777 - output_classification_loss: 0.8984\n",
            "Epoch 263/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4627 - output_regression_loss: 0.5624 - output_classification_loss: 0.9003\n",
            "Epoch 264/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4797 - output_regression_loss: 0.5762 - output_classification_loss: 0.9035\n",
            "Epoch 265/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4704 - output_regression_loss: 0.5598 - output_classification_loss: 0.9105\n",
            "Epoch 266/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4705 - output_regression_loss: 0.5705 - output_classification_loss: 0.8999\n",
            "Epoch 267/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4673 - output_regression_loss: 0.5695 - output_classification_loss: 0.8979\n",
            "Epoch 268/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4984 - output_regression_loss: 0.6042 - output_classification_loss: 0.8942\n",
            "Epoch 269/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4966 - output_regression_loss: 0.5831 - output_classification_loss: 0.9134\n",
            "Epoch 270/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5019 - output_regression_loss: 0.5937 - output_classification_loss: 0.9081\n",
            "Epoch 271/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4871 - output_regression_loss: 0.5859 - output_classification_loss: 0.9012\n",
            "Epoch 272/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4917 - output_regression_loss: 0.5758 - output_classification_loss: 0.9159\n",
            "Epoch 273/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4756 - output_regression_loss: 0.5758 - output_classification_loss: 0.8997\n",
            "Epoch 274/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4399 - output_regression_loss: 0.5539 - output_classification_loss: 0.8860\n",
            "Epoch 275/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4604 - output_regression_loss: 0.5703 - output_classification_loss: 0.8901\n",
            "Epoch 276/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4874 - output_regression_loss: 0.5907 - output_classification_loss: 0.8967\n",
            "Epoch 277/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4884 - output_regression_loss: 0.5933 - output_classification_loss: 0.8951\n",
            "Epoch 278/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4600 - output_regression_loss: 0.5592 - output_classification_loss: 0.9008\n",
            "Epoch 279/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4729 - output_regression_loss: 0.5720 - output_classification_loss: 0.9009\n",
            "Epoch 280/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4857 - output_regression_loss: 0.5894 - output_classification_loss: 0.8963\n",
            "Epoch 281/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4662 - output_regression_loss: 0.5779 - output_classification_loss: 0.8883\n",
            "Epoch 282/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5262 - output_regression_loss: 0.6293 - output_classification_loss: 0.8969\n",
            "Epoch 283/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4696 - output_regression_loss: 0.5752 - output_classification_loss: 0.8944\n",
            "Epoch 284/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4748 - output_regression_loss: 0.5687 - output_classification_loss: 0.9062\n",
            "Epoch 285/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5199 - output_regression_loss: 0.6065 - output_classification_loss: 0.9135\n",
            "Epoch 286/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4842 - output_regression_loss: 0.5956 - output_classification_loss: 0.8886\n",
            "Epoch 287/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4823 - output_regression_loss: 0.5774 - output_classification_loss: 0.9049\n",
            "Epoch 288/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4743 - output_regression_loss: 0.5648 - output_classification_loss: 0.9095\n",
            "Epoch 289/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5002 - output_regression_loss: 0.5825 - output_classification_loss: 0.9177\n",
            "Epoch 290/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4806 - output_regression_loss: 0.5783 - output_classification_loss: 0.9023\n",
            "Epoch 291/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4808 - output_regression_loss: 0.5846 - output_classification_loss: 0.8962\n",
            "Epoch 292/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4757 - output_regression_loss: 0.5698 - output_classification_loss: 0.9059\n",
            "Epoch 293/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4456 - output_regression_loss: 0.5573 - output_classification_loss: 0.8882\n",
            "Epoch 294/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4555 - output_regression_loss: 0.5618 - output_classification_loss: 0.8937\n",
            "Epoch 295/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5048 - output_regression_loss: 0.5942 - output_classification_loss: 0.9106\n",
            "Epoch 296/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4871 - output_regression_loss: 0.5947 - output_classification_loss: 0.8924\n",
            "Epoch 297/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4761 - output_regression_loss: 0.5706 - output_classification_loss: 0.9055\n",
            "Epoch 298/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4674 - output_regression_loss: 0.5733 - output_classification_loss: 0.8941\n",
            "Epoch 299/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4856 - output_regression_loss: 0.5712 - output_classification_loss: 0.9143\n",
            "Epoch 300/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4586 - output_regression_loss: 0.5534 - output_classification_loss: 0.9052\n",
            "Epoch 301/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4768 - output_regression_loss: 0.5768 - output_classification_loss: 0.9000\n",
            "Epoch 302/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4880 - output_regression_loss: 0.5738 - output_classification_loss: 0.9143\n",
            "Epoch 303/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4883 - output_regression_loss: 0.5706 - output_classification_loss: 0.9177\n",
            "Epoch 304/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4844 - output_regression_loss: 0.5798 - output_classification_loss: 0.9047\n",
            "Epoch 305/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4766 - output_regression_loss: 0.5831 - output_classification_loss: 0.8935\n",
            "Epoch 306/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4588 - output_regression_loss: 0.5623 - output_classification_loss: 0.8966\n",
            "Epoch 307/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4766 - output_regression_loss: 0.5851 - output_classification_loss: 0.8914\n",
            "Epoch 308/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4734 - output_regression_loss: 0.5684 - output_classification_loss: 0.9050\n",
            "Epoch 309/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4525 - output_regression_loss: 0.5459 - output_classification_loss: 0.9066\n",
            "Epoch 310/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5012 - output_regression_loss: 0.6102 - output_classification_loss: 0.8910\n",
            "Epoch 311/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4656 - output_regression_loss: 0.5598 - output_classification_loss: 0.9058\n",
            "Epoch 312/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4996 - output_regression_loss: 0.6045 - output_classification_loss: 0.8952\n",
            "Epoch 313/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4860 - output_regression_loss: 0.5874 - output_classification_loss: 0.8986\n",
            "Epoch 314/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4805 - output_regression_loss: 0.5664 - output_classification_loss: 0.9141\n",
            "Epoch 315/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5214 - output_regression_loss: 0.6183 - output_classification_loss: 0.9031\n",
            "Epoch 316/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4880 - output_regression_loss: 0.5963 - output_classification_loss: 0.8917\n",
            "Epoch 317/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4454 - output_regression_loss: 0.5551 - output_classification_loss: 0.8903\n",
            "Epoch 318/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4769 - output_regression_loss: 0.5710 - output_classification_loss: 0.9058\n",
            "Epoch 319/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4705 - output_regression_loss: 0.5773 - output_classification_loss: 0.8932\n",
            "Epoch 320/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4787 - output_regression_loss: 0.5717 - output_classification_loss: 0.9070\n",
            "Epoch 321/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4844 - output_regression_loss: 0.5804 - output_classification_loss: 0.9040\n",
            "Epoch 322/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4668 - output_regression_loss: 0.5802 - output_classification_loss: 0.8866\n",
            "Epoch 323/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4711 - output_regression_loss: 0.5873 - output_classification_loss: 0.8838\n",
            "Epoch 324/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4885 - output_regression_loss: 0.5857 - output_classification_loss: 0.9029\n",
            "Epoch 325/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4278 - output_regression_loss: 0.5445 - output_classification_loss: 0.8833\n",
            "Epoch 326/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4963 - output_regression_loss: 0.5896 - output_classification_loss: 0.9068\n",
            "Epoch 327/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4844 - output_regression_loss: 0.5702 - output_classification_loss: 0.9142\n",
            "Epoch 328/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4701 - output_regression_loss: 0.5838 - output_classification_loss: 0.8863\n",
            "Epoch 329/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4772 - output_regression_loss: 0.5742 - output_classification_loss: 0.9030\n",
            "Epoch 330/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4526 - output_regression_loss: 0.5573 - output_classification_loss: 0.8953\n",
            "Epoch 331/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4802 - output_regression_loss: 0.5704 - output_classification_loss: 0.9098\n",
            "Epoch 332/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4916 - output_regression_loss: 0.5823 - output_classification_loss: 0.9094\n",
            "Epoch 333/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4710 - output_regression_loss: 0.5594 - output_classification_loss: 0.9117\n",
            "Epoch 334/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4867 - output_regression_loss: 0.5849 - output_classification_loss: 0.9019\n",
            "Epoch 335/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4988 - output_regression_loss: 0.6076 - output_classification_loss: 0.8912\n",
            "Epoch 336/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4561 - output_regression_loss: 0.5587 - output_classification_loss: 0.8974\n",
            "Epoch 337/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4766 - output_regression_loss: 0.5793 - output_classification_loss: 0.8972\n",
            "Epoch 338/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5125 - output_regression_loss: 0.5919 - output_classification_loss: 0.9206\n",
            "Epoch 339/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4664 - output_regression_loss: 0.5693 - output_classification_loss: 0.8971\n",
            "Epoch 340/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4659 - output_regression_loss: 0.5586 - output_classification_loss: 0.9074\n",
            "Epoch 341/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4683 - output_regression_loss: 0.5763 - output_classification_loss: 0.8919\n",
            "Epoch 342/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4787 - output_regression_loss: 0.5760 - output_classification_loss: 0.9027\n",
            "Epoch 343/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4744 - output_regression_loss: 0.5775 - output_classification_loss: 0.8969\n",
            "Epoch 344/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4981 - output_regression_loss: 0.6098 - output_classification_loss: 0.8883\n",
            "Epoch 345/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4976 - output_regression_loss: 0.5818 - output_classification_loss: 0.9158\n",
            "Epoch 346/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4583 - output_regression_loss: 0.5720 - output_classification_loss: 0.8862\n",
            "Epoch 347/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4871 - output_regression_loss: 0.5695 - output_classification_loss: 0.9176\n",
            "Epoch 348/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4740 - output_regression_loss: 0.5747 - output_classification_loss: 0.8994\n",
            "Epoch 349/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4942 - output_regression_loss: 0.5952 - output_classification_loss: 0.8990\n",
            "Epoch 350/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4600 - output_regression_loss: 0.5707 - output_classification_loss: 0.8893\n",
            "Epoch 351/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4886 - output_regression_loss: 0.5994 - output_classification_loss: 0.8892\n",
            "Epoch 352/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4546 - output_regression_loss: 0.5500 - output_classification_loss: 0.9047\n",
            "Epoch 353/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4986 - output_regression_loss: 0.5866 - output_classification_loss: 0.9120\n",
            "Epoch 354/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4808 - output_regression_loss: 0.5770 - output_classification_loss: 0.9038\n",
            "Epoch 355/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4609 - output_regression_loss: 0.5633 - output_classification_loss: 0.8976\n",
            "Epoch 356/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5127 - output_regression_loss: 0.6130 - output_classification_loss: 0.8997\n",
            "Epoch 357/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5084 - output_regression_loss: 0.5938 - output_classification_loss: 0.9146\n",
            "Epoch 358/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4720 - output_regression_loss: 0.5697 - output_classification_loss: 0.9022\n",
            "Epoch 359/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4941 - output_regression_loss: 0.5746 - output_classification_loss: 0.9195\n",
            "Epoch 360/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4696 - output_regression_loss: 0.5780 - output_classification_loss: 0.8916\n",
            "Epoch 361/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4683 - output_regression_loss: 0.5665 - output_classification_loss: 0.9018\n",
            "Epoch 362/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4760 - output_regression_loss: 0.5878 - output_classification_loss: 0.8882\n",
            "Epoch 363/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4693 - output_regression_loss: 0.5695 - output_classification_loss: 0.8999\n",
            "Epoch 364/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4884 - output_regression_loss: 0.5752 - output_classification_loss: 0.9132\n",
            "Epoch 365/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4791 - output_regression_loss: 0.5782 - output_classification_loss: 0.9009\n",
            "Epoch 366/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4823 - output_regression_loss: 0.5789 - output_classification_loss: 0.9034\n",
            "Epoch 367/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4891 - output_regression_loss: 0.5885 - output_classification_loss: 0.9006\n",
            "Epoch 368/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4897 - output_regression_loss: 0.5887 - output_classification_loss: 0.9010\n",
            "Epoch 369/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4337 - output_regression_loss: 0.5479 - output_classification_loss: 0.8857\n",
            "Epoch 370/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4743 - output_regression_loss: 0.5717 - output_classification_loss: 0.9026\n",
            "Epoch 371/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4650 - output_regression_loss: 0.5651 - output_classification_loss: 0.8999\n",
            "Epoch 372/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4588 - output_regression_loss: 0.5589 - output_classification_loss: 0.8999\n",
            "Epoch 373/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4907 - output_regression_loss: 0.5903 - output_classification_loss: 0.9004\n",
            "Epoch 374/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4969 - output_regression_loss: 0.5934 - output_classification_loss: 0.9035\n",
            "Epoch 375/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4822 - output_regression_loss: 0.5832 - output_classification_loss: 0.8991\n",
            "Epoch 376/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4907 - output_regression_loss: 0.5863 - output_classification_loss: 0.9044\n",
            "Epoch 377/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5091 - output_regression_loss: 0.6030 - output_classification_loss: 0.9061\n",
            "Epoch 378/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4857 - output_regression_loss: 0.5778 - output_classification_loss: 0.9079\n",
            "Epoch 379/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4786 - output_regression_loss: 0.5740 - output_classification_loss: 0.9046\n",
            "Epoch 380/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5117 - output_regression_loss: 0.5900 - output_classification_loss: 0.9217\n",
            "Epoch 381/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4678 - output_regression_loss: 0.5705 - output_classification_loss: 0.8974\n",
            "Epoch 382/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4952 - output_regression_loss: 0.5971 - output_classification_loss: 0.8981\n",
            "Epoch 383/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4648 - output_regression_loss: 0.5828 - output_classification_loss: 0.8819\n",
            "Epoch 384/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4847 - output_regression_loss: 0.5883 - output_classification_loss: 0.8964\n",
            "Epoch 385/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4903 - output_regression_loss: 0.5771 - output_classification_loss: 0.9132\n",
            "Epoch 386/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4851 - output_regression_loss: 0.5940 - output_classification_loss: 0.8911\n",
            "Epoch 387/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5119 - output_regression_loss: 0.5954 - output_classification_loss: 0.9165\n",
            "Epoch 388/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5246 - output_regression_loss: 0.6084 - output_classification_loss: 0.9162\n",
            "Epoch 389/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4817 - output_regression_loss: 0.5875 - output_classification_loss: 0.8942\n",
            "Epoch 390/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4705 - output_regression_loss: 0.5743 - output_classification_loss: 0.8963\n",
            "Epoch 391/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4555 - output_regression_loss: 0.5625 - output_classification_loss: 0.8931\n",
            "Epoch 392/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4662 - output_regression_loss: 0.5698 - output_classification_loss: 0.8964\n",
            "Epoch 393/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4597 - output_regression_loss: 0.5670 - output_classification_loss: 0.8928\n",
            "Epoch 394/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4471 - output_regression_loss: 0.5587 - output_classification_loss: 0.8884\n",
            "Epoch 395/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4487 - output_regression_loss: 0.5680 - output_classification_loss: 0.8807\n",
            "Epoch 396/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4767 - output_regression_loss: 0.5706 - output_classification_loss: 0.9061\n",
            "Epoch 397/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4578 - output_regression_loss: 0.5706 - output_classification_loss: 0.8872\n",
            "Epoch 398/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4669 - output_regression_loss: 0.5663 - output_classification_loss: 0.9006\n",
            "Epoch 399/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4909 - output_regression_loss: 0.5762 - output_classification_loss: 0.9147\n",
            "Epoch 400/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4871 - output_regression_loss: 0.5956 - output_classification_loss: 0.8915\n",
            "Epoch 401/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4848 - output_regression_loss: 0.5565 - output_classification_loss: 0.9283\n",
            "Epoch 402/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4567 - output_regression_loss: 0.5544 - output_classification_loss: 0.9024\n",
            "Epoch 403/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5057 - output_regression_loss: 0.5872 - output_classification_loss: 0.9186\n",
            "Epoch 404/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4826 - output_regression_loss: 0.5873 - output_classification_loss: 0.8953\n",
            "Epoch 405/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4752 - output_regression_loss: 0.5701 - output_classification_loss: 0.9051\n",
            "Epoch 406/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4530 - output_regression_loss: 0.5668 - output_classification_loss: 0.8862\n",
            "Epoch 407/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4829 - output_regression_loss: 0.5799 - output_classification_loss: 0.9031\n",
            "Epoch 408/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4903 - output_regression_loss: 0.5801 - output_classification_loss: 0.9103\n",
            "Epoch 409/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4665 - output_regression_loss: 0.5615 - output_classification_loss: 0.9051\n",
            "Epoch 410/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4568 - output_regression_loss: 0.5535 - output_classification_loss: 0.9033\n",
            "Epoch 411/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4768 - output_regression_loss: 0.5665 - output_classification_loss: 0.9104\n",
            "Epoch 412/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4612 - output_regression_loss: 0.5552 - output_classification_loss: 0.9060\n",
            "Epoch 413/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5031 - output_regression_loss: 0.5954 - output_classification_loss: 0.9077\n",
            "Epoch 414/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5008 - output_regression_loss: 0.5911 - output_classification_loss: 0.9097\n",
            "Epoch 415/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4787 - output_regression_loss: 0.5758 - output_classification_loss: 0.9029\n",
            "Epoch 416/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4939 - output_regression_loss: 0.5849 - output_classification_loss: 0.9090\n",
            "Epoch 417/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4703 - output_regression_loss: 0.5668 - output_classification_loss: 0.9035\n",
            "Epoch 418/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4631 - output_regression_loss: 0.5687 - output_classification_loss: 0.8944\n",
            "Epoch 419/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5159 - output_regression_loss: 0.6000 - output_classification_loss: 0.9160\n",
            "Epoch 420/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4762 - output_regression_loss: 0.5784 - output_classification_loss: 0.8978\n",
            "Epoch 421/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4764 - output_regression_loss: 0.5751 - output_classification_loss: 0.9013\n",
            "Epoch 422/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4673 - output_regression_loss: 0.5606 - output_classification_loss: 0.9066\n",
            "Epoch 423/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5262 - output_regression_loss: 0.5903 - output_classification_loss: 0.9359\n",
            "Epoch 424/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4870 - output_regression_loss: 0.5848 - output_classification_loss: 0.9022\n",
            "Epoch 425/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4925 - output_regression_loss: 0.5890 - output_classification_loss: 0.9035\n",
            "Epoch 426/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4890 - output_regression_loss: 0.5897 - output_classification_loss: 0.8993\n",
            "Epoch 427/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4838 - output_regression_loss: 0.5733 - output_classification_loss: 0.9104\n",
            "Epoch 428/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4630 - output_regression_loss: 0.5648 - output_classification_loss: 0.8982\n",
            "Epoch 429/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4852 - output_regression_loss: 0.5792 - output_classification_loss: 0.9060\n",
            "Epoch 430/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4610 - output_regression_loss: 0.5539 - output_classification_loss: 0.9071\n",
            "Epoch 431/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4860 - output_regression_loss: 0.6068 - output_classification_loss: 0.8792\n",
            "Epoch 432/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4993 - output_regression_loss: 0.5861 - output_classification_loss: 0.9133\n",
            "Epoch 433/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5044 - output_regression_loss: 0.5952 - output_classification_loss: 0.9092\n",
            "Epoch 434/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4970 - output_regression_loss: 0.5921 - output_classification_loss: 0.9049\n",
            "Epoch 435/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4601 - output_regression_loss: 0.5588 - output_classification_loss: 0.9013\n",
            "Epoch 436/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4648 - output_regression_loss: 0.5756 - output_classification_loss: 0.8893\n",
            "Epoch 437/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4657 - output_regression_loss: 0.5545 - output_classification_loss: 0.9112\n",
            "Epoch 438/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4866 - output_regression_loss: 0.5797 - output_classification_loss: 0.9069\n",
            "Epoch 439/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4895 - output_regression_loss: 0.5829 - output_classification_loss: 0.9067\n",
            "Epoch 440/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4813 - output_regression_loss: 0.5662 - output_classification_loss: 0.9151\n",
            "Epoch 441/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4465 - output_regression_loss: 0.5455 - output_classification_loss: 0.9009\n",
            "Epoch 442/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4486 - output_regression_loss: 0.5449 - output_classification_loss: 0.9037\n",
            "Epoch 443/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5291 - output_regression_loss: 0.6019 - output_classification_loss: 0.9273\n",
            "Epoch 444/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4816 - output_regression_loss: 0.5801 - output_classification_loss: 0.9015\n",
            "Epoch 445/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4666 - output_regression_loss: 0.5806 - output_classification_loss: 0.8860\n",
            "Epoch 446/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4771 - output_regression_loss: 0.5736 - output_classification_loss: 0.9035\n",
            "Epoch 447/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4450 - output_regression_loss: 0.5567 - output_classification_loss: 0.8882\n",
            "Epoch 448/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5031 - output_regression_loss: 0.5923 - output_classification_loss: 0.9107\n",
            "Epoch 449/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4842 - output_regression_loss: 0.5840 - output_classification_loss: 0.9002\n",
            "Epoch 450/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4915 - output_regression_loss: 0.5917 - output_classification_loss: 0.8998\n",
            "Epoch 451/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5172 - output_regression_loss: 0.5885 - output_classification_loss: 0.9287\n",
            "Epoch 452/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4592 - output_regression_loss: 0.5601 - output_classification_loss: 0.8991\n",
            "Epoch 453/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4598 - output_regression_loss: 0.5688 - output_classification_loss: 0.8910\n",
            "Epoch 454/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5010 - output_regression_loss: 0.5840 - output_classification_loss: 0.9171\n",
            "Epoch 455/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4482 - output_regression_loss: 0.5564 - output_classification_loss: 0.8918\n",
            "Epoch 456/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4650 - output_regression_loss: 0.5596 - output_classification_loss: 0.9054\n",
            "Epoch 457/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4912 - output_regression_loss: 0.5987 - output_classification_loss: 0.8925\n",
            "Epoch 458/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4533 - output_regression_loss: 0.5504 - output_classification_loss: 0.9029\n",
            "Epoch 459/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4770 - output_regression_loss: 0.5691 - output_classification_loss: 0.9079\n",
            "Epoch 460/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4800 - output_regression_loss: 0.5880 - output_classification_loss: 0.8920\n",
            "Epoch 461/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4656 - output_regression_loss: 0.5747 - output_classification_loss: 0.8909\n",
            "Epoch 462/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4706 - output_regression_loss: 0.5604 - output_classification_loss: 0.9102\n",
            "Epoch 463/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4692 - output_regression_loss: 0.5558 - output_classification_loss: 0.9134\n",
            "Epoch 464/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4621 - output_regression_loss: 0.5757 - output_classification_loss: 0.8864\n",
            "Epoch 465/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4653 - output_regression_loss: 0.5740 - output_classification_loss: 0.8914\n",
            "Epoch 466/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4932 - output_regression_loss: 0.5830 - output_classification_loss: 0.9102\n",
            "Epoch 467/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4618 - output_regression_loss: 0.5708 - output_classification_loss: 0.8910\n",
            "Epoch 468/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4629 - output_regression_loss: 0.5695 - output_classification_loss: 0.8934\n",
            "Epoch 469/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4884 - output_regression_loss: 0.5878 - output_classification_loss: 0.9006\n",
            "Epoch 470/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4842 - output_regression_loss: 0.5772 - output_classification_loss: 0.9070\n",
            "Epoch 471/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4669 - output_regression_loss: 0.5662 - output_classification_loss: 0.9007\n",
            "Epoch 472/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4535 - output_regression_loss: 0.5536 - output_classification_loss: 0.8999\n",
            "Epoch 473/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4685 - output_regression_loss: 0.5814 - output_classification_loss: 0.8871\n",
            "Epoch 474/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4516 - output_regression_loss: 0.5560 - output_classification_loss: 0.8956\n",
            "Epoch 475/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4532 - output_regression_loss: 0.5599 - output_classification_loss: 0.8934\n",
            "Epoch 476/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4854 - output_regression_loss: 0.5994 - output_classification_loss: 0.8860\n",
            "Epoch 477/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4694 - output_regression_loss: 0.5604 - output_classification_loss: 0.9089\n",
            "Epoch 478/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4505 - output_regression_loss: 0.5429 - output_classification_loss: 0.9076\n",
            "Epoch 479/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4683 - output_regression_loss: 0.5644 - output_classification_loss: 0.9039\n",
            "Epoch 480/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4993 - output_regression_loss: 0.5974 - output_classification_loss: 0.9020\n",
            "Epoch 481/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4515 - output_regression_loss: 0.5658 - output_classification_loss: 0.8857\n",
            "Epoch 482/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4444 - output_regression_loss: 0.5375 - output_classification_loss: 0.9069\n",
            "Epoch 483/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4921 - output_regression_loss: 0.5737 - output_classification_loss: 0.9184\n",
            "Epoch 484/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4567 - output_regression_loss: 0.5647 - output_classification_loss: 0.8920\n",
            "Epoch 485/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4865 - output_regression_loss: 0.5872 - output_classification_loss: 0.8993\n",
            "Epoch 486/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4961 - output_regression_loss: 0.5886 - output_classification_loss: 0.9075\n",
            "Epoch 487/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4677 - output_regression_loss: 0.5692 - output_classification_loss: 0.8985\n",
            "Epoch 488/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4668 - output_regression_loss: 0.5553 - output_classification_loss: 0.9115\n",
            "Epoch 489/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4680 - output_regression_loss: 0.5546 - output_classification_loss: 0.9134\n",
            "Epoch 490/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4931 - output_regression_loss: 0.5891 - output_classification_loss: 0.9040\n",
            "Epoch 491/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4442 - output_regression_loss: 0.5596 - output_classification_loss: 0.8846\n",
            "Epoch 492/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4712 - output_regression_loss: 0.5743 - output_classification_loss: 0.8969\n",
            "Epoch 493/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4247 - output_regression_loss: 0.5321 - output_classification_loss: 0.8926\n",
            "Epoch 494/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4724 - output_regression_loss: 0.5800 - output_classification_loss: 0.8924\n",
            "Epoch 495/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4849 - output_regression_loss: 0.5661 - output_classification_loss: 0.9188\n",
            "Epoch 496/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4769 - output_regression_loss: 0.5924 - output_classification_loss: 0.8844\n",
            "Epoch 497/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4986 - output_regression_loss: 0.5887 - output_classification_loss: 0.9100\n",
            "Epoch 498/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4805 - output_regression_loss: 0.5819 - output_classification_loss: 0.8986\n",
            "Epoch 499/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4375 - output_regression_loss: 0.5519 - output_classification_loss: 0.8856\n",
            "Epoch 500/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4749 - output_regression_loss: 0.5600 - output_classification_loss: 0.9149\n",
            "Epoch 501/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4785 - output_regression_loss: 0.5597 - output_classification_loss: 0.9188\n",
            "Epoch 502/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5220 - output_regression_loss: 0.6013 - output_classification_loss: 0.9207\n",
            "Epoch 503/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5021 - output_regression_loss: 0.5791 - output_classification_loss: 0.9230\n",
            "Epoch 504/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5005 - output_regression_loss: 0.5928 - output_classification_loss: 0.9077\n",
            "Epoch 505/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4785 - output_regression_loss: 0.5789 - output_classification_loss: 0.8996\n",
            "Epoch 506/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4846 - output_regression_loss: 0.5767 - output_classification_loss: 0.9078\n",
            "Epoch 507/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4584 - output_regression_loss: 0.5735 - output_classification_loss: 0.8849\n",
            "Epoch 508/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4889 - output_regression_loss: 0.5765 - output_classification_loss: 0.9123\n",
            "Epoch 509/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4819 - output_regression_loss: 0.5816 - output_classification_loss: 0.9002\n",
            "Epoch 510/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4587 - output_regression_loss: 0.5694 - output_classification_loss: 0.8892\n",
            "Epoch 511/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4763 - output_regression_loss: 0.5783 - output_classification_loss: 0.8979\n",
            "Epoch 512/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4644 - output_regression_loss: 0.5613 - output_classification_loss: 0.9031\n",
            "Epoch 513/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5035 - output_regression_loss: 0.5897 - output_classification_loss: 0.9138\n",
            "Epoch 514/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4535 - output_regression_loss: 0.5706 - output_classification_loss: 0.8829\n",
            "Epoch 515/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4758 - output_regression_loss: 0.5836 - output_classification_loss: 0.8922\n",
            "Epoch 516/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5321 - output_regression_loss: 0.6146 - output_classification_loss: 0.9175\n",
            "Epoch 517/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4800 - output_regression_loss: 0.5816 - output_classification_loss: 0.8983\n",
            "Epoch 518/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4814 - output_regression_loss: 0.5802 - output_classification_loss: 0.9012\n",
            "Epoch 519/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4570 - output_regression_loss: 0.5687 - output_classification_loss: 0.8883\n",
            "Epoch 520/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4878 - output_regression_loss: 0.5828 - output_classification_loss: 0.9050\n",
            "Epoch 521/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4601 - output_regression_loss: 0.5672 - output_classification_loss: 0.8929\n",
            "Epoch 522/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4500 - output_regression_loss: 0.5573 - output_classification_loss: 0.8927\n",
            "Epoch 523/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4853 - output_regression_loss: 0.5791 - output_classification_loss: 0.9062\n",
            "Epoch 524/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4681 - output_regression_loss: 0.5703 - output_classification_loss: 0.8978\n",
            "Epoch 525/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4822 - output_regression_loss: 0.5889 - output_classification_loss: 0.8934\n",
            "Epoch 526/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4723 - output_regression_loss: 0.5562 - output_classification_loss: 0.9161\n",
            "Epoch 527/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4951 - output_regression_loss: 0.5917 - output_classification_loss: 0.9035\n",
            "Epoch 528/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4649 - output_regression_loss: 0.5727 - output_classification_loss: 0.8922\n",
            "Epoch 529/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4748 - output_regression_loss: 0.5874 - output_classification_loss: 0.8874\n",
            "Epoch 530/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4946 - output_regression_loss: 0.6050 - output_classification_loss: 0.8896\n",
            "Epoch 531/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4875 - output_regression_loss: 0.5892 - output_classification_loss: 0.8983\n",
            "Epoch 532/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4694 - output_regression_loss: 0.5704 - output_classification_loss: 0.8989\n",
            "Epoch 533/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4702 - output_regression_loss: 0.5762 - output_classification_loss: 0.8941\n",
            "Epoch 534/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5072 - output_regression_loss: 0.5852 - output_classification_loss: 0.9221\n",
            "Epoch 535/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4774 - output_regression_loss: 0.5798 - output_classification_loss: 0.8976\n",
            "Epoch 536/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4908 - output_regression_loss: 0.5609 - output_classification_loss: 0.9299\n",
            "Epoch 537/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4875 - output_regression_loss: 0.5832 - output_classification_loss: 0.9044\n",
            "Epoch 538/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4742 - output_regression_loss: 0.5757 - output_classification_loss: 0.8985\n",
            "Epoch 539/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4646 - output_regression_loss: 0.5745 - output_classification_loss: 0.8900\n",
            "Epoch 540/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4614 - output_regression_loss: 0.5454 - output_classification_loss: 0.9160\n",
            "Epoch 541/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4848 - output_regression_loss: 0.5760 - output_classification_loss: 0.9088\n",
            "Epoch 542/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4875 - output_regression_loss: 0.5701 - output_classification_loss: 0.9173\n",
            "Epoch 543/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4830 - output_regression_loss: 0.5860 - output_classification_loss: 0.8971\n",
            "Epoch 544/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4621 - output_regression_loss: 0.5592 - output_classification_loss: 0.9030\n",
            "Epoch 545/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4810 - output_regression_loss: 0.5748 - output_classification_loss: 0.9062\n",
            "Epoch 546/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4638 - output_regression_loss: 0.5651 - output_classification_loss: 0.8988\n",
            "Epoch 547/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4586 - output_regression_loss: 0.5722 - output_classification_loss: 0.8864\n",
            "Epoch 548/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4855 - output_regression_loss: 0.5783 - output_classification_loss: 0.9071\n",
            "Epoch 549/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4689 - output_regression_loss: 0.5704 - output_classification_loss: 0.8985\n",
            "Epoch 550/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4688 - output_regression_loss: 0.5717 - output_classification_loss: 0.8971\n",
            "Epoch 551/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4702 - output_regression_loss: 0.5683 - output_classification_loss: 0.9019\n",
            "Epoch 552/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4612 - output_regression_loss: 0.5559 - output_classification_loss: 0.9053\n",
            "Epoch 553/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4501 - output_regression_loss: 0.5424 - output_classification_loss: 0.9077\n",
            "Epoch 554/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4704 - output_regression_loss: 0.5587 - output_classification_loss: 0.9116\n",
            "Epoch 555/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4764 - output_regression_loss: 0.5721 - output_classification_loss: 0.9043\n",
            "Epoch 556/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4757 - output_regression_loss: 0.5828 - output_classification_loss: 0.8929\n",
            "Epoch 557/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4587 - output_regression_loss: 0.5630 - output_classification_loss: 0.8956\n",
            "Epoch 558/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4812 - output_regression_loss: 0.5795 - output_classification_loss: 0.9017\n",
            "Epoch 559/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4992 - output_regression_loss: 0.5913 - output_classification_loss: 0.9079\n",
            "Epoch 560/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4742 - output_regression_loss: 0.5753 - output_classification_loss: 0.8989\n",
            "Epoch 561/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4671 - output_regression_loss: 0.5710 - output_classification_loss: 0.8961\n",
            "Epoch 562/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4844 - output_regression_loss: 0.5860 - output_classification_loss: 0.8983\n",
            "Epoch 563/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4864 - output_regression_loss: 0.5676 - output_classification_loss: 0.9188\n",
            "Epoch 564/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.5380 - output_regression_loss: 0.6099 - output_classification_loss: 0.9281\n",
            "Epoch 565/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4732 - output_regression_loss: 0.5751 - output_classification_loss: 0.8981\n",
            "Epoch 566/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5147 - output_regression_loss: 0.6062 - output_classification_loss: 0.9086\n",
            "Epoch 567/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4901 - output_regression_loss: 0.5904 - output_classification_loss: 0.8997\n",
            "Epoch 568/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4539 - output_regression_loss: 0.5606 - output_classification_loss: 0.8933\n",
            "Epoch 569/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4647 - output_regression_loss: 0.5610 - output_classification_loss: 0.9038\n",
            "Epoch 570/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4605 - output_regression_loss: 0.5680 - output_classification_loss: 0.8924\n",
            "Epoch 571/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4759 - output_regression_loss: 0.5718 - output_classification_loss: 0.9041\n",
            "Epoch 572/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4700 - output_regression_loss: 0.5930 - output_classification_loss: 0.8770\n",
            "Epoch 573/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4773 - output_regression_loss: 0.5819 - output_classification_loss: 0.8953\n",
            "Epoch 574/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4900 - output_regression_loss: 0.5960 - output_classification_loss: 0.8940\n",
            "Epoch 575/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4673 - output_regression_loss: 0.5668 - output_classification_loss: 0.9005\n",
            "Epoch 576/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5103 - output_regression_loss: 0.5824 - output_classification_loss: 0.9279\n",
            "Epoch 577/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4829 - output_regression_loss: 0.5788 - output_classification_loss: 0.9040\n",
            "Epoch 578/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4975 - output_regression_loss: 0.5905 - output_classification_loss: 0.9070\n",
            "Epoch 579/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4649 - output_regression_loss: 0.5728 - output_classification_loss: 0.8922\n",
            "Epoch 580/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4717 - output_regression_loss: 0.5809 - output_classification_loss: 0.8908\n",
            "Epoch 581/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4398 - output_regression_loss: 0.5542 - output_classification_loss: 0.8856\n",
            "Epoch 582/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4853 - output_regression_loss: 0.5748 - output_classification_loss: 0.9105\n",
            "Epoch 583/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4808 - output_regression_loss: 0.5979 - output_classification_loss: 0.8829\n",
            "Epoch 584/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4897 - output_regression_loss: 0.5897 - output_classification_loss: 0.9000\n",
            "Epoch 585/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4681 - output_regression_loss: 0.5569 - output_classification_loss: 0.9112\n",
            "Epoch 586/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5165 - output_regression_loss: 0.5838 - output_classification_loss: 0.9326\n",
            "Epoch 587/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4888 - output_regression_loss: 0.5835 - output_classification_loss: 0.9053\n",
            "Epoch 588/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4604 - output_regression_loss: 0.5630 - output_classification_loss: 0.8974\n",
            "Epoch 589/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4628 - output_regression_loss: 0.5678 - output_classification_loss: 0.8950\n",
            "Epoch 590/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4829 - output_regression_loss: 0.5781 - output_classification_loss: 0.9048\n",
            "Epoch 591/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4698 - output_regression_loss: 0.5712 - output_classification_loss: 0.8986\n",
            "Epoch 592/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4706 - output_regression_loss: 0.5719 - output_classification_loss: 0.8987\n",
            "Epoch 593/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4996 - output_regression_loss: 0.6062 - output_classification_loss: 0.8934\n",
            "Epoch 594/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4609 - output_regression_loss: 0.5516 - output_classification_loss: 0.9093\n",
            "Epoch 595/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4761 - output_regression_loss: 0.5705 - output_classification_loss: 0.9056\n",
            "Epoch 596/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4600 - output_regression_loss: 0.5649 - output_classification_loss: 0.8951\n",
            "Epoch 597/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4975 - output_regression_loss: 0.5844 - output_classification_loss: 0.9130\n",
            "Epoch 598/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4678 - output_regression_loss: 0.5878 - output_classification_loss: 0.8800\n",
            "Epoch 599/600\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 1.4870 - output_regression_loss: 0.5805 - output_classification_loss: 0.9065\n",
            "Epoch 600/600\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4489 - output_regression_loss: 0.5465 - output_classification_loss: 0.9024\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb67b682ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAiPft6eDCih",
        "outputId": "9e143e9d-6d15-4bc7-b64f-0f88193b12bd"
      },
      "source": [
        "test_one = norm_test_x[0:][:1]\r\n",
        "model.predict(test_one)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[9.612353]], dtype=float32),\n",
              " array([[0.38806   , 0.25256836, 0.35937163]], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxvbUDGwEzHX",
        "outputId": "74b1a8c2-007d-4e48-9532-f7b6830d9087"
      },
      "source": [
        "test_y[0][0], test_y[1][0]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "tETtWZQpI_4j",
        "outputId": "266eb9b7-551a-478b-901d-e2cc19696f58"
      },
      "source": [
        "plot_model(model)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    }
  ]
}