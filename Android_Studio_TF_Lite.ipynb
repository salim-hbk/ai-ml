{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Android_Studio_TF_Lite.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNHcHp9btjNtHxhnX5e4FHI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salim-hbk/ai-ml/blob/main/Android_Studio_TF_Lite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_743O7YC9HuM"
      },
      "source": [
        "# Adding tensorflow Lite to Android Studio Project:\r\n",
        "http://bintray.com/google/tensorflow/tensorflow-lite\r\n",
        "```\r\n",
        "dependencies{\r\n",
        "  implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n",
        "}\r\n",
        ".\r\n",
        "'\r\n",
        "'\r\n",
        "android{\r\n",
        "    aaptOptions{\r\n",
        "        noCompress : 'tflite'\r\n",
        "    }\r\n",
        "}\r\n",
        "\r\n",
        "```\r\n",
        "\r\n",
        "The steps involved in using model in Mobile devices\r\n",
        "1. Initialize the model\r\n",
        "2. Prrepare the input image as the image input expected by the Mobilenet model is 224x224x3\r\n",
        "3. Perform the inference.\r\n",
        "4. Map the result.\r\n",
        "\r\n",
        "When using TFlite model on Mobile devices. it allows us to use couple of configurational parameters.\r\n",
        "\r\n",
        "1) setNumThreads(int count)\r\n",
        "2) setUseNNAPI(boolean useNNApi) //usage of hardware accelerator neural network api\r\n",
        "3) setAllowFp16PrecisionForFp32(boolean use)\r\n",
        "4) addDelegate(boolean delegate) // use GPU \r\n",
        "\r\n",
        "E.g.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "```\r\n",
        "val tfliteOptions = Interpreter.Options();\r\n",
        "tfliteOptions.(3)\r\n",
        "```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNEhlINBBk3T"
      },
      "source": [
        "# Loading models and Labels.\r\n",
        "\r\n",
        "Model tensorflow lite file is saved in asset folder. to load the file use below code:\r\n",
        "```\r\n",
        "val fileDecriptor = assetManager.openFd('converted_model.tflite')\r\n",
        "val inputStream = FileInputStream(fileDecriptor.fileDecriptor)\r\n",
        "val fileChannel  = inputStream.channel\r\n",
        "val startOffset = inputStream.startOffset\r\n",
        "val declaredLength = inputStream.declaredLength\r\n",
        "//Load TFLite model.\r\n",
        "tfLiteModel = fileChannel.map(FileChannel.Map.READ_ONLY, startOffset, declaredLength)\r\n",
        "\r\n",
        "// Setting the label\r\n",
        "val label = listOf('cat', 'dog')\r\n",
        "interpreter = Interpreter(tfLiteModel, tfliteOptions)\r\n",
        "\r\n",
        "```\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtqZUg9BMrQB"
      },
      "source": [
        "# Preparing the input is not as easy as i python\r\n",
        "it has following cases to be handled.\r\n",
        "\r\n",
        "\r\n",
        "*   Images taken from camera has larger resoltion 120x740, where as imagenet needs 224x224\r\n",
        "*   Images captured from Camera has ARGB vales, where as image net expects 3 channels\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "```\r\n",
        "Solutions:\r\n",
        "1. Bitmap.createScaledBitmap(bitmap, inputsize, inputsize, false)\r\n",
        "2. \r\n",
        "val byteBuffer = ByteBuffer.allocateDirect(batch_size*inputsize*inputsize*pixelsize)\r\n",
        "byteBuffer.order(ByteOrder.nativeOrder())\r\n",
        "then we can use byte masking technique to get the individual channels from the image\r\n",
        "val red = (input.shr(16) and 0xff)\r\n",
        "val green = (input.shr(8) and 0xff)\r\n",
        "val blue = (input and 0xff)\r\n",
        "\r\n",
        "```\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB0NPzo-9tv6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}